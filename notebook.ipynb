{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Article dependacies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T01:17:25.955079Z",
     "iopub.status.busy": "2022-09-02T01:17:25.954697Z",
     "iopub.status.idle": "2022-09-02T01:17:26.122559Z",
     "shell.execute_reply": "2022-09-02T01:17:26.121854Z",
     "shell.execute_reply.started": "2022-09-02T01:17:25.955008Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import Request\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T01:17:27.304920Z",
     "iopub.status.busy": "2022-09-02T01:17:27.304134Z",
     "iopub.status.idle": "2022-09-02T01:17:27.310500Z",
     "shell.execute_reply": "2022-09-02T01:17:27.309851Z",
     "shell.execute_reply.started": "2022-09-02T01:17:27.304889Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#  create html object\n",
    "html = \"\"\"\n",
    "<html>\n",
    "    <title>Mock Webpage</title>\n",
    "    <body>\n",
    "        <h1>Web Scraping</h1>\n",
    "        \n",
    "        <p>This article is all about web scraping</p>\n",
    "\n",
    "        <p>We will be using BeautifulSoup</p>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "#  create beautifulsoup element\n",
    "bs = BeautifulSoup(html, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T01:17:36.614617Z",
     "iopub.status.busy": "2022-09-02T01:17:36.613965Z",
     "iopub.status.idle": "2022-09-02T01:17:36.626167Z",
     "shell.execute_reply": "2022-09-02T01:17:36.625194Z",
     "shell.execute_reply.started": "2022-09-02T01:17:36.614583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mock Webpage'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  extract the title tag\n",
    "bs.title\n",
    "#  extract the h1 tag\n",
    "bs.h1\n",
    "#  extract the p tag\n",
    "bs.p\n",
    "#  extract all p tags\n",
    "bs.find_all('p')\n",
    "#  extract only the string in the title tag\n",
    "bs.title.get_text()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup & Web Page Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T01:18:09.895363Z",
     "iopub.status.busy": "2022-09-02T01:18:09.894606Z",
     "iopub.status.idle": "2022-09-02T01:18:10.163370Z",
     "shell.execute_reply": "2022-09-02T01:18:10.161987Z",
     "shell.execute_reply.started": "2022-09-02T01:18:09.895332Z"
    }
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.request import Request\n",
    "\n",
    "url = 'https://blog.paperspace.com/generating-images-with-stable-diffusion/'\n",
    "\n",
    "#  header to mimick web browser\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "           'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "           'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "           'Accept-Encoding': 'none',\n",
    "           'Accept-Language': 'en-US,en;q=0.8',\n",
    "           'Connection': 'keep-alive'}\n",
    "\n",
    "#  make request to server\n",
    "request = Request(url, headers=headers)\n",
    "\n",
    "#  open request and create beautifulsoup element\n",
    "html = urlopen(request)\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tags and attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T01:18:12.526451Z",
     "iopub.status.busy": "2022-09-02T01:18:12.525860Z",
     "iopub.status.idle": "2022-09-02T01:18:13.914306Z",
     "shell.execute_reply": "2022-09-02T01:18:13.913632Z",
     "shell.execute_reply.started": "2022-09-02T01:18:12.526423Z"
    }
   },
   "outputs": [],
   "source": [
    "#  copy link from page 2 and edit 2 to 1 to access the first page\n",
    "url = 'https://www.jumia.com.ng/mlp-fashion-deals/mens-athletic-shoes/?page=1#catalog-listing'\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "      'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "      'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "      'Accept-Encoding': 'none',\n",
    "      'Accept-Language': 'en-US,en;q=0.8',\n",
    "      'Connection': 'keep-alive'}\n",
    "\n",
    "request = Request(url, headers=headers)\n",
    "\n",
    "html = urlopen(request)\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "\n",
    "#  extract all img tags with class img\n",
    "interest = bs.find_all('img', attrs={'class':'img'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  extracting links using list comprehension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T01:19:37.551471Z",
     "iopub.status.busy": "2022-09-02T01:19:37.550654Z",
     "iopub.status.idle": "2022-09-02T01:19:37.554573Z",
     "shell.execute_reply": "2022-09-02T01:19:37.553973Z",
     "shell.execute_reply.started": "2022-09-02T01:19:37.551442Z"
    }
   },
   "outputs": [],
   "source": [
    "#  extracting links using list comprehension\n",
    "links = [listing['data-src'] for listing in interest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading from src Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T01:19:38.666284Z",
     "iopub.status.busy": "2022-09-02T01:19:38.665418Z",
     "iopub.status.idle": "2022-09-02T01:19:55.799619Z",
     "shell.execute_reply": "2022-09-02T01:19:55.798602Z",
     "shell.execute_reply.started": "2022-09-02T01:19:38.666251Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:17<00:00,  2.34it/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "#  instantiating counter\n",
    "count = 0\n",
    "\n",
    "#  downloading images\n",
    "for link in tqdm(links):\n",
    "  with open(f'athletic_{count}.jpg', 'wb') as f:\n",
    "    response = requests.get(link)\n",
    "    image = response.content\n",
    "    f.write(image)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T01:19:55.802142Z",
     "iopub.status.busy": "2022-09-02T01:19:55.801859Z",
     "iopub.status.idle": "2022-09-02T01:19:55.812612Z",
     "shell.execute_reply": "2022-09-02T01:19:55.811597Z",
     "shell.execute_reply.started": "2022-09-02T01:19:55.802117Z"
    }
   },
   "outputs": [],
   "source": [
    "class WebScraper():\n",
    "    def __init__(self, headers, tag: str, attribute: dict,\n",
    "                 src_attribute: str, filepath: str, count=0):\n",
    "      self.headers = headers\n",
    "      self.tag = tag\n",
    "      self.attribute = attribute\n",
    "      self.src_attribute = src_attribute\n",
    "      self.filepath = filepath\n",
    "      self.count = count\n",
    "      self.bs = []\n",
    "      self.interest = []\n",
    "\n",
    "    def __str__(self):\n",
    "      display = f\"\"\"      CLASS ATTRIBUTES\n",
    "      headers: headers used so as to mimic requests coming from web browsers.\n",
    "      tag: html tags intended for scraping.\n",
    "      attribute: attributes of the html tags of interest.\n",
    "      filepath: path ending with filenames to use when scraping images.\n",
    "      count: numerical suffix to differentiate files in the same folder.\n",
    "      bs: a list of each page's beautifulsoup elements.\n",
    "      interest: a list of each page's image links.\"\"\"\n",
    "      return display\n",
    "\n",
    "    def __repr__(self):\n",
    "      display = f\"\"\"      CLASS ATTRIBUTES\n",
    "      headers: {self.headers}\n",
    "      tag: {self.tag}\n",
    "      attribute: {self.attribute}\n",
    "      filepath: {self.filepath}\n",
    "      count: {self.count}\n",
    "      bs: {self.bs}\n",
    "      interest: {self.interest}\"\"\"\n",
    "      return display\n",
    "\n",
    "    def parse_html(self, url):\n",
    "      \"\"\"\n",
    "      This method requests the webpage from the server and\n",
    "      returns a beautifulsoup element\n",
    "      \"\"\"\n",
    "      try:\n",
    "        request = Request(url, headers=self.headers)\n",
    "        html = urlopen(request)\n",
    "        bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "        self.bs.append(bs)\n",
    "      except Exception as e:\n",
    "        print(f'problem with webpage\\n{e}')\n",
    "      pass\n",
    "\n",
    "    def extract_src(self):\n",
    "      \"\"\"\n",
    "      This method extracts tags of interest from the webpage's\n",
    "      html\n",
    "      \"\"\"\n",
    "      #  extracting tag of interest\n",
    "      interest = self.bs[-1].find_all(self.tag, attrs=self.attribute)\n",
    "      interest = [listing[self.src_attribute] for listing in interest]\n",
    "      self.interest.append(interest)\n",
    "      pass\n",
    "\n",
    "    def scrape_images(self):\n",
    "      \"\"\"\n",
    "      This method grabs images located in the src links and\n",
    "      saves them as required\n",
    "      \"\"\"\n",
    "      for link in tqdm(self.interest[-1]):\n",
    "        try:\n",
    "          with open(f'{self.filepath}_{self.count}.jpg', 'wb') as f:\n",
    "            response = requests.get(link)\n",
    "            image = response.content\n",
    "            f.write(image)\n",
    "            self.count += 1\n",
    "            #  pausing scraping for 0.4secs so as to not exceed 200 requests per minute as stipulated in the web page's robots.txt file\n",
    "            time.sleep(0.4)\n",
    "        except Exception as e:\n",
    "          print(f'problem with image\\n{e}')\n",
    "          time.sleep(0.4)\n",
    "      pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T01:19:55.814405Z",
     "iopub.status.busy": "2022-09-02T01:19:55.814182Z",
     "iopub.status.idle": "2022-09-02T01:19:55.818482Z",
     "shell.execute_reply": "2022-09-02T01:19:55.817667Z",
     "shell.execute_reply.started": "2022-09-02T01:19:55.814386Z"
    }
   },
   "outputs": [],
   "source": [
    "#  instantiating web scraper class\n",
    "scraper = WebScraper(headers=headers, tag='img', attribute={'class': 'img'},\n",
    "                     src_attribute='data-src', filepath='shoes/athletic/atl', count=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T01:19:55.820291Z",
     "iopub.status.busy": "2022-09-02T01:19:55.820057Z",
     "iopub.status.idle": "2022-09-02T01:19:55.826281Z",
     "shell.execute_reply": "2022-09-02T01:19:55.824703Z",
     "shell.execute_reply.started": "2022-09-02T01:19:55.820273Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_scraper(scraper, page_range: list):\n",
    "    \"\"\"\n",
    "    This function wraps around the web scraper class allowing it to scrape\n",
    "    multiple pages. The argument page_range takes both a list of two elements\n",
    "    to define a range of pages or a list of one element to define a single page.\n",
    "    \"\"\"\n",
    "    if len(page_range) > 1:\n",
    "      for i in range(page_range[0], page_range[1] + 1):\n",
    "        scraper.parse_html(\n",
    "            url=f'https://www.jumia.com.ng/mlp-fashion-deals/mens-athletic-shoes/?page={i}#catalog-listing')\n",
    "        scraper.extract_src()\n",
    "        scraper.scrape_images()\n",
    "        print(f'\\npage {i} done.')\n",
    "      print('All Done!')\n",
    "    else:\n",
    "      scraper.parse_html(\n",
    "          url=f'https://www.jumia.com.ng/mlp-fashion-deals/mens-athletic-shoes/?page={page_range[0]}#catalog-listing')\n",
    "      scraper.extract_src()\n",
    "      scraper.scrape_images()\n",
    "      print('\\nAll Done!')\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T01:19:55.827560Z",
     "iopub.status.busy": "2022-09-02T01:19:55.827358Z",
     "iopub.status.idle": "2022-09-02T01:20:35.913400Z",
     "shell.execute_reply": "2022-09-02T01:20:35.912877Z",
     "shell.execute_reply.started": "2022-09-02T01:19:55.827543Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:26<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "page 1 done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:10<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "page 2 done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "page 3 done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "page 4 done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "page 5 done.\n",
      "All Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#  creating directory to hold images\n",
    "os.mkdir('shoes')\n",
    "os.mkdir('shoes/athletic')\n",
    "\n",
    "#  scraping the first five pages\n",
    "my_scraper(scraper=scraper, page_range=[1, 5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
