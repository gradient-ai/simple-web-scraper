{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#  Article dependacies\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import Request\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import os\n",
    "import time\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#  create html object\n",
    "html = \"\"\"\n",
    "<html>\n",
    "    <title>Mock Webpage</title>\n",
    "    <body>\n",
    "        <h1>Web Scraping</h1>\n",
    "        \n",
    "        <p>This article is all about web scraping</p>\n",
    "\n",
    "        <p>We will be using BeautifulSoup</p>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "#  create beautifulsoup element\n",
    "bs = BeautifulSoup(html, 'html.parser')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  extract the title tag\n",
    "bs.title\n",
    "#  extract the h1 tag\n",
    "bs.h1\n",
    "#  extract the p tag\n",
    "bs.p\n",
    "#  extract all p tags\n",
    "bs.find_all(p)\n",
    "#  extract only the string in the title tag\n",
    "bs.title.get_text()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BeautifulSoup & Web Page Scraping"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.request import Request\n",
    "\n",
    "url = 'actual link'\n",
    "\n",
    "#  header to mimick web browser\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "           'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "           'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "           'Accept-Encoding': 'none',\n",
    "           'Accept-Language': 'en-US,en;q=0.8',\n",
    "           'Connection': 'keep-alive'}\n",
    "\n",
    "#  make request to server\n",
    "request = Request(url, headers=headers)\n",
    "\n",
    "#  open request and create beautifulsoup element\n",
    "html = urlopen(request)\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# tags and attributes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  copy link from page 2 and edit 2 to 1 to access the first page\n",
    "url = 'https://www.jumia.com.ng/mlp-fashion-deals/mens-athletic-shoes/?page=1#catalog-listing'\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "      'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "      'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "      'Accept-Encoding': 'none',\n",
    "      'Accept-Language': 'en-US,en;q=0.8',\n",
    "      'Connection': 'keep-alive'}\n",
    "\n",
    "request = Request(url, headers=headers)\n",
    "\n",
    "html = urlopen(request)\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "\n",
    "#  extract all img tags with class img\n",
    "interest = bs.find_all('img', attrs={'class':'img'})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  extracting links using list comprehension\n",
    "links = [listing['data-src'] for listing in interest]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Downloading from src Links"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import requests\n",
    "\n",
    "#  instantiating counter\n",
    "count = 0\n",
    "\n",
    "#  downloading images\n",
    "for link in tqdm(links):\n",
    "  with open(f'athletic_{count}.jpg', 'wb') as f:\n",
    "    response = requests.get(link)\n",
    "    image = response.content\n",
    "    f.write(image)\n",
    "    count+=1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class WebScraper():\n",
    "    def __init__(self, headers, tag: str, attribute: dict,\n",
    "                 src_attribute: str, filepath: str, count=0):\n",
    "      self.headers = headers\n",
    "      self.tag = tag\n",
    "      self.attribute = attribute\n",
    "      self.src_attribute = src_attribute\n",
    "      self.filepath = filepath\n",
    "      self.count = count\n",
    "      self.bs = []\n",
    "      self.interest = []\n",
    "\n",
    "    def __str__(self):\n",
    "      display = f\"\"\"      CLASS ATTRIBUTES\n",
    "      headers: headers used so as to mimic requests coming from web browsers.\n",
    "      tag: html tags intended for scraping.\n",
    "      attribute: attributes of the html tags of interest.\n",
    "      filepath: path ending with filenames to use when scraping images.\n",
    "      count: numerical suffix to differentiate files in the same folder.\n",
    "      bs: a list of each page's beautifulsoup elements.\n",
    "      interest: a list of each page's image links.\"\"\"\n",
    "      return display\n",
    "\n",
    "    def __repr__(self):\n",
    "      display = f\"\"\"      CLASS ATTRIBUTES\n",
    "      headers: {self.headers}\n",
    "      tag: {self.tag}\n",
    "      attribute: {self.attribute}\n",
    "      filepath: {self.filepath}\n",
    "      count: {self.count}\n",
    "      bs: {self.bs}\n",
    "      interest: {self.interest}\"\"\"\n",
    "      return display\n",
    "\n",
    "    def parse_html(self, url):\n",
    "      \"\"\"\n",
    "      This method requests the webpage from the server and\n",
    "      returns a beautifulsoup element\n",
    "      \"\"\"\n",
    "      try:\n",
    "        request = Request(url, headers=self.headers)\n",
    "        html = urlopen(request)\n",
    "        bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "        self.bs.append(bs)\n",
    "      except Exception as e:\n",
    "        print(f'problem with webpage\\n{e}')\n",
    "      pass\n",
    "\n",
    "    def extract_src(self):\n",
    "      \"\"\"\n",
    "      This method extracts tags of interest from the webpage's\n",
    "      html\n",
    "      \"\"\"\n",
    "      #  extracting tag of interest\n",
    "      interest = self.bs[-1].find_all(self.tag, attrs=self.attribute)\n",
    "      interest = [listing[self.src_attribute] for listing in interest]\n",
    "      self.interest.append(interest)\n",
    "      pass\n",
    "\n",
    "    def scrape_images(self):\n",
    "      \"\"\"\n",
    "      This method grabs images located in the src links and\n",
    "      saves them as required\n",
    "      \"\"\"\n",
    "      for link in tqdm(self.interest[-1]):\n",
    "        try:\n",
    "          with open(f'{self.filepath}_{self.count}.jpg', 'wb') as f:\n",
    "            response = requests.get(link)\n",
    "            image = response.content\n",
    "            f.write(image)\n",
    "            self.count += 1\n",
    "            #  pausing scraping for 0.4secs so as to not exceed 200 requests per minute as stipulated in the web page's robots.txt file\n",
    "            time.sleep(0.4)\n",
    "        except Exception as e:\n",
    "          print(f'problem with image\\n{e}')\n",
    "          time.sleep(0.4)\n",
    "      pass\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  instantiating web scraper class\n",
    "scraper = WebScraper(headers=headers, tag='img', attribute={'class': 'img'},\n",
    "                     src_attribute='data-src', filepath='shoes/athletic/atl', count=0)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def my_scraper(scraper, page_range: list):\n",
    "    \"\"\"\n",
    "    This function wraps around the web scraper class allowing it to scrape\n",
    "    multiple pages. The argument page_range takes both a list of two elements\n",
    "    to define a range of pages or a list of one element to define a single page.\n",
    "    \"\"\"\n",
    "    if len(page_range) > 1:\n",
    "      for i in range(page_range[0], page_range[1] + 1):\n",
    "        scraper.parse_html(\n",
    "            url=f'https://www.jumia.com.ng/mlp-fashion-deals/mens-athletic-shoes/?page={i}#catalog-listing')\n",
    "        scraper.extract_src()\n",
    "        scraper.scrape_images()\n",
    "        print(f'\\npage {i} done.')\n",
    "      print('All Done!')\n",
    "    else:\n",
    "      scraper.parse_html(\n",
    "          url=f'https://www.jumia.com.ng/mlp-fashion-deals/mens-athletic-shoes/?page={page_range[0]}#catalog-listing')\n",
    "      scraper.extract_src()\n",
    "      scraper.scrape_images()\n",
    "      print('\\nAll Done!')\n",
    "    pass\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "#  creating directory to hold images\n",
    "os.mkdir('shoes')\n",
    "os.mkdir('shoes/athletic')\n",
    "\n",
    "#  scraping the first five pages\n",
    "my_scraper(scraper=scraper, page_range=[1, 5])\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}