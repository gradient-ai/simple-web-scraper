{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  article dependencies\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import Request\n",
    "import time\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class WebScraper():\n",
    "    def __init__(self, headers, tag: str, attribute: dict,\n",
    "                 src_attribute: str, filepath: str, count=0):\n",
    "      self.headers = headers\n",
    "      self.tag = tag\n",
    "      self.attribute = attribute\n",
    "      self.src_attribute = src_attribute\n",
    "      self.filepath = filepath\n",
    "      self.count = count\n",
    "      self.bs = []\n",
    "      self.interest = []\n",
    "\n",
    "    def __str__(self):\n",
    "      display = f\"\"\"      CLASS ATTRIBUTES\n",
    "      headers: headers used so as to mimic requests coming from web browsers.\n",
    "      tag: html tags intended for scraping.\n",
    "      attribute: attributes of the html tags of interest.\n",
    "      filepath: path ending with filenames to use when scraping images.\n",
    "      count: numerical suffix to differentiate files in the same folder.\n",
    "      bs: a list of each page's beautifulsoup elements.\n",
    "      interest: a list of each page's image links.\"\"\"\n",
    "      return display\n",
    "\n",
    "    def __repr__(self):\n",
    "      display = f\"\"\"      CLASS ATTRIBUTES\n",
    "      headers: {self.headers}\n",
    "      tag: {self.tag}\n",
    "      attribute: {self.attribute}\n",
    "      filepath: {self.filepath}\n",
    "      count: {self.count}\n",
    "      bs: {self.bs}\n",
    "      interest: {self.interest}\"\"\"\n",
    "      return display\n",
    "\n",
    "    def parse_html(self, url):\n",
    "      \"\"\"\n",
    "      This method requests the webpage from the server and\n",
    "      returns a beautifulsoup element\n",
    "      \"\"\"\n",
    "      try:\n",
    "        request = Request(url, headers=self.headers)\n",
    "        html = urlopen(request)\n",
    "        bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "        self.bs.append(bs)\n",
    "      except Exception as e:\n",
    "        print(f'problem with webpage\\n{e}')\n",
    "      pass\n",
    "\n",
    "    def extract_src(self):\n",
    "      \"\"\"\n",
    "      This method extracts tags of interest from the webpage's\n",
    "      html\n",
    "      \"\"\"\n",
    "      #  extracting tag of interest\n",
    "      interest = self.bs[-1].find_all(self.tag, attrs=self.attribute)\n",
    "      interest = [listing[self.src_attribute] for listing in interest]\n",
    "      self.interest.append(interest)\n",
    "      pass\n",
    "\n",
    "    def scrape_images(self):\n",
    "      \"\"\"\n",
    "      This method grabs images located in the src links and\n",
    "      saves them as required\n",
    "      \"\"\"\n",
    "      for link in tqdm(self.interest[-1]):\n",
    "        try:\n",
    "          with open(f'{self.filepath}_{self.count}.jpg', 'wb') as f:\n",
    "            response = requests.get(link)\n",
    "            image = response.content\n",
    "            f.write(image)\n",
    "            self.count += 1\n",
    "            time.sleep(0.4)\n",
    "        except Exception as e:\n",
    "          print(f'problem with image\\n{e}')\n",
    "          time.sleep(0.4)\n",
    "      pass\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def my_scraper(scraper, page_range: list):\n",
    "    \"\"\"\n",
    "    This function wraps around the web scraper class allowing it to scrape\n",
    "    multiple pages. The argument page_range takes both a list of two elements\n",
    "    to define a range of pages or a list of one element to define a single page.\n",
    "    \"\"\"\n",
    "    if len(page_range) > 1:\n",
    "      for i in range(page_range[0], page_range[1] + 1):\n",
    "        scraper.parse_html(\n",
    "            url=f'https://www.jumia.com.ng/mlp-fashion-deals/mens-athletic-shoes/?page={i}#catalog-listing')\n",
    "        scraper.extract_src()\n",
    "        scraper.scrape_images()\n",
    "        print(f'\\npage {i} done.')\n",
    "      print('All Done!')\n",
    "    else:\n",
    "      scraper.parse_html(\n",
    "          url=f'https://www.jumia.com.ng/mlp-fashion-deals/mens-athletic-shoes/?page={page_range[0]}#catalog-listing')\n",
    "      scraper.extract_src()\n",
    "      scraper.scrape_images()\n",
    "      print('\\nAll Done!')\n",
    "    pass\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  create directories to hold images\n",
    "os.mkdir('shoes')\n",
    "os.mkdir('shoes/athletic')\n",
    "os.mkdir('shoes/boots')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scrape the images"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "           'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "           'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "           'Accept-Encoding': 'none',\n",
    "           'Accept-Language': 'en-US,en;q=0.8',\n",
    "           'Connection': 'keep-alive'}\n",
    "\n",
    "#  scrape athletic shoe images\n",
    "athletic_scraper = WebScraper(headers=headers, tag='img', attribute={'class': 'img'},\n",
    "                              src_attribute='data-src', filepath='shoes/athletic/atl', count=0)\n",
    "\n",
    "my_scraper(scraper=athletic_scraper, page_range=[1, 3])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  replace the urls in the my scraper function with the urls below\n",
    "#  first url:\n",
    "#  f'https://www.jumia.com.ng/mlp-fashion-deals/mens-boots/?page={i}#catalog-listing'\n",
    "#  second url:\n",
    "#  f'https://www.jumia.com.ng/mlp-fashion-deals/mens-boots/?page={page_range[0]}#catalog-listing'\n",
    "#  rerun my_scraper function code cell\n",
    "\n",
    "#  scrape boot images\n",
    "boot_scraper = WebScraper(headers=headers, tag='img', attribute={'class': 'img'},\n",
    "                          src_attribute='data-src', filepath='shoes/boots/boot', count=0)\n",
    "\n",
    "my_scraper(scraper=boot_scraper, page_range=[1, 3])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load & Label data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  defining class to load and label data\n",
    "class LoadShoeData():\n",
    "    \"\"\"\n",
    "    This class loads in data from each directory in numpy array format then saves\n",
    "    loaded dataset\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.athletic = 'shoes/athletic'\n",
    "        self.boots = 'shoes/boots'\n",
    "        self.labels = {self.athletic: np.eye(2, 2)[0], self.boots: np.eye(2, 2)[1]}\n",
    "        self.img_size = 100\n",
    "        self.dataset = []\n",
    "        self.athletic_count = 0\n",
    "        self.boots_count = 0\n",
    "\n",
    "    def create_dataset(self):\n",
    "        \"\"\"\n",
    "        This method reads images as grayscale from directories,\n",
    "        resizes them and labels them as required.\n",
    "        \"\"\"\n",
    "\n",
    "        #  reading from directory\n",
    "        for key in self.labels:\n",
    "          print(key)\n",
    "\n",
    "          #  looping through all files in the directory\n",
    "          for img_file in tqdm(os.listdir(key)):\n",
    "            try:\n",
    "              #  deriving image path\n",
    "              path = os.path.join(key, img_file)\n",
    "\n",
    "              #  reading image\n",
    "              image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "              image = cv2.resize(image, (self.img_size, self.img_size))\n",
    "\n",
    "              #  appending image and class label to list\n",
    "              self.dataset.append([image, self.labels[key]])\n",
    "\n",
    "              #  incrementing counter\n",
    "              if key == self.athletic:\n",
    "                self.athletic_count+=1\n",
    "              elif key == self.boots:\n",
    "                self.boots_count+=1\n",
    "\n",
    "            except Exception as e:\n",
    "              pass\n",
    "\n",
    "        #  shuffling array of images\n",
    "        np.random.shuffle(self.dataset)\n",
    "\n",
    "        #  printing to screen\n",
    "        print(f'\\nathletic shoe images: {self.athletic_count}')\n",
    "        print(f'boot images: {self.boots_count}')\n",
    "        print(f'total: {self.athletic_count + self.boots_count}')\n",
    "        print('All done!')\n",
    "        return np.array(self.dataset, dtype='object')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  load data\n",
    "data = LoadShoeData()\n",
    "\n",
    "dataset = data.create_dataset()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating a PyTorch dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  extending Dataset class\n",
    "class ShoeDataset(Dataset):\n",
    "    def __init__(self, custom_dataset, transforms=None):\n",
    "        self.custom_dataset = custom_dataset\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.custom_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #  extracting image from index and scaling\n",
    "        image = self.custom_dataset[idx][0]\n",
    "        #  extracting label from index\n",
    "        label = torch.tensor(self.custom_dataset[idx][1])\n",
    "        #  applying transforms if transforms are supplied\n",
    "        if self.transforms:\n",
    "          image = self.transforms(image)\n",
    "        return (image, label)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  creating an instance of the dataset class\n",
    "dataset = ShoeDataset(dataset, transforms=transforms.ToTensor())\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}